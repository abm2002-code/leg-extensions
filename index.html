<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Leg Raises Pose Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils"></script>
    <style>
        body { text-align: center; font-family: Arial, sans-serif; }
        video, canvas { position: absolute; left: 50%; transform: translateX(-50%); top: 10%; }
        #warning { position: absolute; top: 5%; left: 50%; transform: translateX(-50%); color: red; font-size: 24px; font-weight: bold; }
        #debug { position: absolute; top: 80%; left: 10%; color: white; font-size: 14px; text-align: left; }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
    <div id="warning"></div>
    <div id="debug"></div>
    <audio id="audio" src="leg_raises.mp3" preload="auto"></audio>
    
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const warningText = document.getElementById('warning');
        const debugText = document.getElementById('debug');
        const audio = document.getElementById('audio');
        
        // Load the model (in a real implementation, you would load a trained model here)
        // For this demo, we'll use a simple threshold-based approach
        
        const pose = new Pose({ locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}` });
        pose.setOptions({ modelComplexity: 1, smoothLandmarks: true, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5 });

        navigator.mediaDevices.getUserMedia({ video: true }).then(stream => {
            video.srcObject = stream;
            video.onloadedmetadata = () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
            };
        });

        function calculateAngle(a, b, c) {
            const ba = { x: a.x - b.x, y: a.y - b.y };
            const bc = { x: c.x - b.x, y: c.y - b.y };
            const dotProduct = ba.x * bc.x + ba.y * bc.y;
            const magnitudeBA = Math.sqrt(ba.x ** 2 + ba.y ** 2);
            const magnitudeBC = Math.sqrt(bc.x ** 2 + bc.y ** 2);
            const angle = Math.acos(dotProduct / (magnitudeBA * magnitudeBC)) * (180 / Math.PI);
            return angle;
        }

        pose.onResults(results => {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            if (results.poseLandmarks) {
                // Draw pose landmarks and connections
                drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#00FF00', lineWidth: 4 });
                drawLandmarks(ctx, results.poseLandmarks, { color: '#FF0000', lineWidth: 2 });

                const landmarks = results.poseLandmarks;
                
                // Get relevant landmarks for right knee angle
                const rightHip = landmarks[24]; // RIGHT_HIP
                const rightKnee = landmarks[26]; // RIGHT_KNEE
                const rightAnkle = landmarks[28]; // RIGHT_ANKLE
                
                // Calculate knee angle for the right leg
                const kneeAngle = calculateAngle(rightHip, rightKnee, rightAnkle);
                
                // Display the angle on screen
                ctx.fillStyle = "white";
                ctx.font = "18px Arial";
                ctx.fillText(`Right Knee Angle: ${Math.round(kneeAngle)}Â°`, 50, 50);
                
                // Simple classification based on knee angle
                // In a real implementation, you would use the trained model here
                let classification = "correct";
                if (kneeAngle < 160) { // Example threshold
                    classification = "incorrect";
                }
                
                ctx.fillText(`Movement classification: ${classification}`, 50, 80);
                
                // Play audio if the movement is classified as incorrect
                if (classification === "incorrect" && audio.paused) {
                    warningText.innerText = "Incorrect Leg Raise Form!";
                    audio.play();
                } else if (classification === "correct") {
                    warningText.innerText = "";
                }
                
                // Debug information
                debugText.innerText = `Knee Angle: ${Math.round(kneeAngle)}\nClassification: ${classification}`;
            }
        });

        const camera = new Camera(video, {
            onFrame: async () => { await pose.send({ image: video }); },
            width: 640,
            height: 480
        });
        camera.start();
    </script>
</body>
</html>
